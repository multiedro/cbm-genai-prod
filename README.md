# üìÇ Vis√£o Geral das Etapas

Este reposit√≥rio cont√©m duas etapas principais que utilizam tecnologias do Google Cloud para processamento e an√°lise de documentos.

# ‚öôÔ∏è Etapa 1: Conversor de Formatos de Arquivos com Apache Beam e Dataflow

Esta etapa, encontrada na pasta *conversor_formatos*, utiliza o Apache Beam para orquestrar um pipeline de processamento de dados em lote no Google Cloud Dataflow. A principal fun√ß√£o do pipeline √© monitorar uma pasta no Google Cloud Storage (GCS), converter diversos formatos de arquivo para o formato PDF e salvar os arquivos resultantes em outra pasta no mesmo bucket.

## üèõÔ∏è Vis√£o Geral da Arquitetura

O processo funciona da seguinte maneira:

O script Python `formats_converter.py` define um pipeline do Apache Beam.

Ao ser executado, o script submete o pipeline para o servi√ßo do Google Cloud Dataflow.

O Dataflow provisiona inst√¢ncias de m√°quina virtual (workers) para executar as tarefas de convers√£o.

Cada worker utiliza uma imagem Docker customizada (Dockerfile), que cont√©m todas as depend√™ncias necess√°rias (como LibreOffice e bibliotecas Python) para realizar as convers√µes.

O pipeline l√™ os arquivos de uma pasta de origem no GCS, distribui as tarefas de convers√£o entre os workers e, por fim, salva os PDFs gerados em uma pasta de destino.

## üìÑ Descri√ß√£o dos Arquivos

- `formats_converter.py`: Arquivo principal que cont√©m toda a l√≥gica do pipeline de dados com Apache Beam. Ele lista, filtra e processa arquivos do GCS, utilizando LibreOffice e bibliotecas Python para a convers√£o para PDF.

- `Dockerfile`: Define o ambiente de execu√ß√£o customizado para os workers do Dataflow, instalando o LibreOffice e outras depend√™ncias de sistema e Python.

- `requirements.txt`: Lista as bibliotecas Python necess√°rias para o pipeline de convers√£o, como apache-beam[gcp], Pillow, reportlab, etc.

## üöÄ Como Executar a Etapa

### Pr√©-requisitos

- Um projeto no Google Cloud com as APIs do Dataflow, Cloud Storage e Container Registry ativadas.

- Um bucket no Google Cloud Storage.

- O Google Cloud SDK (gcloud) instalado e autenticado.

- Docker instalado.

### Passos para Execu√ß√£o

- Configurar Vari√°veis: Atualize as vari√°veis globais em formats_converter.py (nome do bucket, pastas, ID do projeto, etc.).

- Construir e Enviar a Imagem Docker:

```
Autenticar o Docker com o gcloud
gcloud auth configure-docker

Construir a imagem Docker
docker build -t gcr.io/[YOUR_PROJECT_ID]/formats_converter:latest .

Enviar a imagem para o Google Container Registry
docker push gcr.io/[YOUR_PROJECT_ID]/formats_converter:latest
```
- Preparar o Ambiente no GCS: Crie as pastas de origem e destino no seu bucket e fa√ßa o upload dos arquivos a serem convertidos.

### Executar o Pipeline:

```python3 formats_converter.py```

O progresso do job pode ser acompanhado na interface do Dataflow no Console do Google Cloud.


# üí¨ Etapa 2: Chatbot de An√°lise de Documentos com Streamlit e Vertex AI

Esta etapa, encontrada na pasta *interface_modelo*, consiste em uma aplica√ß√£o web de chatbot constru√≠da com Streamlit. A aplica√ß√£o permite que os usu√°rios fa√ßam upload de documentos, que s√£o ent√£o indexados no Vertex AI Search. Os usu√°rios podem fazer perguntas em linguagem natural, e o chatbot utiliza um modelo generativo (Gemini) com a t√©cnica de RAG (Retrieval-Augmented Generation) para responder com base no conte√∫do dos documentos.

## üèõÔ∏è Vis√£o Geral da Arquitetura

- A interface do usu√°rio √© constru√≠da com Streamlit (`app.py`, `main.py`).

- Um sistema de autentica√ß√£o (*streamlit-authenticator*) controla o acesso.

- Os usu√°rios fazem upload de arquivos PDF atrav√©s da interface. Os nomes dos arquivos s√£o normalizados (`normalizanome.py`).

- Os arquivos s√£o enviados para um bucket do Google Cloud Storage (`processastorage.py`).

- Um script (`importdocdatastore.py`) √© acionado para indexar os novos documentos no Vertex AI Search (*Data Store*).

- Quando um usu√°rio faz uma pergunta, a aplica√ß√£o primeiro busca documentos relevantes no Data Store (`buscar_documentos.py`).

- A pergunta do usu√°rio, juntamente com o conte√∫do dos documentos relevantes, √© enviada para um modelo Gemini (*Vertex AI*) para gerar uma resposta precisa e contextualizada (`chatvertex.py`).

- A resposta e os links para os documentos de origem s√£o exibidos na interface do chat.

## üìÑ Descri√ß√£o dos Arquivos

- `app.py`: Ponto de entrada da aplica√ß√£o Streamlit. Gerencia a autentica√ß√£o de usu√°rios, a navega√ß√£o e a renderiza√ß√£o das p√°ginas.

- `main.py`: Cont√©m a l√≥gica principal da interface de chat, incluindo upload de arquivos e orquestra√ß√£o da busca e gera√ß√£o de respostas.

- `chatvertex.py`: Interage com a API do Vertex AI para gerar as respostas do chatbot utilizando o modelo Gemini e RAG.

- `buscar_documentos.py`: Utiliza o Vertex AI Search para encontrar os documentos mais relevantes para a pergunta do usu√°rio.

- `importdocdatastore.py`: Inicia a indexa√ß√£o de documentos do GCS para um Data Store do Vertex AI Search.

- `processastorage.py`: Fun√ß√µes utilit√°rias para interagir com o GCS (upload, gera√ß√£o de URLs assinadas).

- `normalizanome.py`: Script auxiliar para padronizar nomes de arquivos antes do upload.

- `Dockerfile`: Define o ambiente para containerizar a aplica√ß√£o Streamlit.

- `requirements.txt`: Lista todas as depend√™ncias Python para a aplica√ß√£o.

- `config_credential.yaml`: Arquivo de configura√ß√£o do streamlit-authenticator, armazenando os detalhes e senhas (hash) dos usu√°rios.

## üöÄ Como Executar a Etapa

### Pr√©-requisitos

- Um projeto no Google Cloud com as APIs Vertex AI Search, Vertex AI e Cloud Storage ativadas.

- Um Data Store configurado no Vertex AI Search.

- Um arquivo de credenciais de Service Account (chave_collavini.json) no diret√≥rio raiz.

- Um arquivo de configura√ß√£o de usu√°rios (config_credential.yaml).

- Docker instalado.

### Passos para Execu√ß√£o

- *Configurar Vari√°veis:* Revise os scripts e atualize os IDs do projeto, Data Store e bucket do GCS conforme necess√°rio.

- Construir e Executar o Cont√™iner Docker:

```
# Construir a imagem Docker
docker build -t cbm-adv:latest .

# Executar o cont√™iner
docker run -p 8080:8080 cbm-adv:latest
```

- *Acessar a Aplica√ß√£o:* Abra seu navegador e acesse http://localhost:8080.

- *Uso:* Fa√ßa o login, realize o upload de documentos PDF pela barra lateral e comece a interagir com o chatbot.
